<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>LoRA on an 8GB GPU — Thesis Study</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content="MS thesis project on parameter-efficient fine-tuning (LoRA) under strict 8 GB GPU constraints."
  />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <div class="page">
    <!-- Top bar -->
    <header class="nav">
      <div class="nav-title">LoRA on an 8GB GPU</div>
      <div class="nav-tag">Thesis study</div>
    </header>

    <!-- Main content -->
    <main class="main">
      <!-- Hero -->
      <section class="hero">
        <div class="badge-row">
          <span class="badge">LoRA / QLoRA</span>
          <span class="badge">8 GB GPU constraints</span>
          <span class="badge">MS thesis</span>
        </div>

        <h1 class="hero-title">LoRA on an 8&nbsp;GB GPU — Thesis Study</h1>
        <p class="hero-subtitle">
          MS thesis project on parameter-efficient fine-tuning (LoRA) under strict 8&nbsp;GB GPU constraints.
        </p>
        <p class="hero-note">
          This page is a human-readable overview of the study design, tracks, and funding context. It links out to
          plans, drafts, and experiments (once they are ready to share).
        </p>
      </section>

      <!-- Study Focus + Tracks -->
      <section class="section-grid">
        <div class="section-card">
          <div class="meta-label">Study Focus</div>
          <p class="meta-block">
            The core study evaluates how LoRA behaves when training and serving models on an 8&nbsp;GB GPU. It examines
            three experimental tracks plus a fourth workflow/cost track.
          </p>

          <div class="details-group" style="margin-top: 1rem;">
            <details open>
              <summary>
                <span class="summary-label">
                  <span class="summary-tag">Track&nbsp;1</span>
                  <span>Text model adaptation</span>
                </span>
                <span class="chevron">▶</span>
              </summary>
              <div class="details-body">
                <p>
                  Small LLMs fine-tuned with LoRA / QLoRA, measuring:
                </p>
                <ul>
                  <li>Task performance (e.g., eval accuracy)</li>
                  <li>Throughput (tokens/sec)</li>
                  <li>Memory behavior (VRAM usage, paging)</li>
                </ul>
                <p>
                  The goal is to see what “respectable” text adaptation looks like on a single 8&nbsp;GB card without
                  full-model fine-tuning.
                </p>
              </div>
            </details>

            <details>
              <summary>
                <span class="summary-label">
                  <span class="summary-tag">Track&nbsp;2</span>
                  <span>Vision model adaptation</span>
                </span>
                <span class="chevron">▶</span>
              </summary>
              <div class="details-body">
                <p>
                  ViT-tiny–scale models trained with LoRA / DoRA to examine:
                </p>
                <ul>
                  <li>Rank and placement tradeoffs</li>
                  <li>Accuracy vs. parameter budget</li>
                  <li>Throughput and memory usage on the same 8&nbsp;GB GPU</li>
                </ul>
                <p>
                  This mirrors the text track but in a compact image setting.
                </p>
              </div>
            </details>

            <details>
              <summary>
                <span class="summary-label">
                  <span class="summary-tag">Track&nbsp;3</span>
                  <span>Multi-adapter serving</span>
                </span>
                <span class="chevron">▶</span>
              </summary>
              <div class="details-body">
                <p>
                  Evaluates serving many task-specific LoRA adapters on a single base model:
                </p>
                <ul>
                  <li>Merged vs. unmerged adapters</li>
                  <li>Latency and time-to-first-token</li>
                  <li>Memory footprint when multiple tasks share one base</li>
                </ul>
                <p>
                  This track treats LoRA as an inference/serving problem, not just training.
                </p>
              </div>
            </details>
          </div>
        </div>

        <!-- Meta / Quick facts -->
        <aside class="section-card">
          <div class="meta-label">Quick facts</div>
          <div class="meta-block">
            <strong>Hardware constraint:</strong> single 8&nbsp;GB GPU (e.g., consumer card).<br />
            <strong>Method:</strong> parameter-efficient fine-tuning (LoRA-family).<br />
            <strong>Scope:</strong> text, vision, and multi-adapter serving.
            <div class="meta-pill-row">
              <span class="meta-pill">Resource-constrained training</span>
              <span class="meta-pill">Practical GPU budgets</span>
              <span class="meta-pill">Serving behavior</span>
            </div>
          </div>
        </aside>
      </section>

      <!-- 4th Track and Funding -->
      <section class="section-grid">
        <div class="section-card">
          <div class="meta-label">4th track</div>

          <details open>
            <summary>
              <span class="summary-label">
                <span class="summary-tag">Track&nbsp;4</span>
                <span>Workflow, cost, and training process</span>
              </span>
              <span class="chevron">▶</span>
            </summary>
            <div class="details-body">
              <p>
                A fourth component evaluates the <strong>training workflow itself</strong>, not just model metrics.
              </p>
              <ul>
                <li>Comparing local 8&nbsp;GB GPU runs to cloud runs funded by credits</li>
                <li>Tracking time, energy use, cost per 10k tokens, and cost per 1% accuracy gain</li>
                <li>
                  Treating the full LoRA lifecycle—local experiment → cloud execution → deployment (e.g., HF/Gradio)—
                  as an object of study
                </li>
              </ul>
              <p>
                This track connects directly to the real-world constraints most practitioners face when they do not
                control large clusters.
              </p>
            </div>
          </details>
        </div>

        <aside class="section-card">
          <div class="meta-label">Funding & compute credits</div>
          <div class="meta-block">
            <p>
              The project currently includes <strong>$150 of Tinker compute credits</strong>, used as seed funding for
              remote experiments.
            </p>
            <p>
              These credits will either:
            </p>
            <ul>
              <li>Be consumed in a focused burst of comparative LoRA trials, or</li>
              <li>Serve as the starting point for extended workflow and cost analysis.</li>
            </ul>
            <p>
              How quickly they are spent—and on which tracks—becomes part of the story about realistic experimentation
              under tight budgets.
            </p>
          </div>
        </aside>
      </section>
    </main>

    <!-- Footer -->
    <footer class="footer">
      LoRA on an 8&nbsp;GB GPU — MS thesis study overview. This site is an evolving summary; details may change as the
      research progresses.
    </footer>
  </div>
</body>
</html>
