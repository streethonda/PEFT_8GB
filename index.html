<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>LoRA on an 8GB GPU — Thesis Study</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content="MS thesis project on parameter-efficient fine-tuning (LoRA) under strict 8 GB GPU constraints."
  />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <div class="page">

    <!-- Top bar -->
    <header class="nav">
      <div class="nav-title">LoRA on an 8GB GPU</div>
      <div class="nav-tag">Thesis study</div>
    </header>

    <!-- Main content -->
    <main class="main">

      <!-- Hero -->
      <section class="hero">
        <div class="badge-row">
          <span class="badge">LoRA / QLoRA</span>
          <span class="badge">8 GB GPU constraints</span>
          <span class="badge">MS thesis</span>
        </div>

        <h1 class="hero-title">LoRA on an 8&nbsp;GB GPU — Thesis Study</h1>
        <p class="hero-subtitle">
          MS thesis project on parameter-efficient fine-tuning (LoRA) under strict 8&nbsp;GB GPU constraints.
        </p>
        <p class="hero-note">
          This page summarizes the study design, tracks, and the funding that supports the project.
        </p>
      </section>

      <!-- Unified Tracks Section -->
      <section class="section-grid">
        <div class="section-card">
          <div class="meta-label">Study Tracks</div>

          <p class="meta-block">
            The study evaluates LoRA behavior under tight 8&nbsp;GB GPU limits across four integrated tracks.
            Track&nbsp;4 exists because <strong>Thinking Machines</strong> provided $150 in research credits,
            enabling workflow and budget analysis beyond the core model-performance tracks.
          </p>

          <div class="details-group" style="margin-top: 1.4rem;">

            <!-- Track 1 -->
            <details open>
              <summary>
                <span class="summary-label">
                  <span class="summary-tag">Track&nbsp;1</span>
                  <span>Text model adaptation</span>
                </span>
                <span class="chevron">▶</span>
              </summary>
              <div class="details-body">
                <ul>
                  <li>Small LLMs fine-tuned with LoRA/QLoRA</li>
                  <li>Task performance (accuracy, eval metrics)</li>
                  <li>Throughput (tokens/sec)</li>
                  <li>VRAM behavior (paging, spills)</li>
                </ul>
              </div>
            </details>

            <!-- Track 2 -->
            <details>
              <summary>
                <span class="summary-label">
                  <span class="summary-tag">Track&nbsp;2</span>
                  <span>Vision model adaptation</span>
                </span>
                <span class="chevron">▶</span>
              </summary>
              <div class="details-body">
                <ul>
                  <li>Training ViT-tiny with LoRA / DoRA</li>
                  <li>Rank & placement tradeoffs</li>
                  <li>Accuracy vs compute/memory use</li>
                </ul>
              </div>
            </details>

            <!-- Track 3 -->
            <details>
              <summary>
                <span class="summary-label">
                  <span class="summary-tag">Track&nbsp;3</span>
                  <span>Multi-adapter serving</span>
                </span>
                <span class="chevron">▶</span>
              </summary>
              <div class="details-body">
                <ul>
                  <li>Serving many LoRA adapters on one base model</li>
                  <li>Merged vs unmerged adapter behavior</li>
                  <li>Latency & time-to-first-token</li>
                </ul>
              </div>
            </details>

            <!-- Track 4 (Funding integrated + geometric PNG) -->
            <details>
              <summary>
                <span class="summary-label">
                  <span class="summary-tag">Track&nbsp;4</span>
                  <span>Workflow, cost, credits, and training process</span>
                </span>
                <span class="chevron">▶</span>
              </summary>

              <div class="details-body">

                <!-- GEOMETRIC PNG IMAGE -->
                <div style="text-align:center; margin-bottom:1.1rem;">
                  <img src="tinker-geom.png"
                       alt="Thinking Machines geometric visual"
                       style="max-width:80%; height:auto; opacity:0.9; border-radius:6px;">
                </div>

                <p>
                  Track 4 analyzes the <strong>end-to-end workflow and financial efficiency</strong> of LoRA training,
                  enabled through <strong>$150 in Tinker compute credits</strong>. It compares local 4060&nbsp;Ti runs
                  against cloud-funded runs on A100 instances or HF Spaces/Gradio jobs.
                </p>

                <p><strong>Core objectives:</strong></p>
                <ul>
                  <li>Measure wall-clock runtime, energy use, and CO₂ footprint</li>
                  <li>Track dollar/credit cost for every experiment (Tracks 1–3)</li>
                  <li>Compute cost-per-10k tokens and cost-per-1% accuracy improvement</li>
                  <li>Determine when local 8&nbsp;GB GPU is preferable vs cloud compute</li>
                </ul>

                <p><strong>Applied demonstration:</strong></p>
                <ul>
                  <li>Prototype a LoRA-tuned image generator</li>
                  <li>Train locally → scale using credits → deploy publicly via HF Spaces</li>
                  <li>Produce a clean, reproducible LoRA workflow for low-budget research</li>
                </ul>

                <p style="margin-top:0.7rem;">
                  Funding link:
                  <a href="https://thinkingmachines.ai/tinker/" target="_blank"
                     style="color:#60a5fa;">Thinking Machines – Tinker</a>
                </p>

              </div>

            </details>

          </div>
        </div>

        <!-- Sidebar -->
        <aside class="section-card">
          <div class="meta-label">References & Documents</div>

          <div class="meta-block">
            <p><strong>Original LoRA Paper:</strong></p>
            <p>
              <a href="https://arxiv.org/abs/2106.09685" target="_blank">
                LoRA: Low-Rank Adaptation of Large Language Models
              </a>
            </p>

            <p style="margin-top: 1.2rem;"><strong>Literature Review (placeholder):</strong></p>
            <p><em>Overview of PEFT and LoRA-related research coming soon.</em></p>
          </div>
        </aside>
      </section>

    </main>

    <!-- Footer -->
    <footer class="footer">
      LoRA on an 8&nbsp;GB GPU — MS thesis study overview. This site is an evolving summary.
    </footer>

  </div>
</body>
</html>
